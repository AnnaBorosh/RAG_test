[
  {
    "title": "Reasoning Models With Recipes",
    "text": "Microsoft published its latest recipe for training reasoning models, substantially expanding what is still a fairly small base of public knowledge.\nWhat’s new:Microsoft releasedPhi-4-reasoning, Phi-4-reasoning-plus, andPhi-4-mini-reasoningalong with lessons learned in building the models.\n- Input/output:text in (Phi-4-reasoning up to 32,000 tokens, Phi–4-reasoning-plus up to 32,000 tokens, Phi-4-mini-reasoning up to 128,000 tokens), text out\n- Architecture:Transformer (Phi-4-reasoning 14 billion parameters, Phi-4-reasoning-plus 14 billion parameters, Phi-4-mini-reasoning: 3.8 billion parameters)\n- Features:Reasoning\n- Performance: Phi-4-reasoning-plus and Phi-4-mini-reasoning perform well on math problems\n- Availability:Weights free todownloadfor noncommercial and commercial uses under anMIT license\nHow it works:All three models are fine-tuned versions of pretrained models.\n- Phi-4-reasoning:The authors fine-tunedPhi-4tomatch curated outputsfromOpenAI o3-minion Q&A, math, science, and coding examples.\n- Phi-4-reasoning-plus:They further fine-tuned Phi-4-reasoning via reinforcement learning to correctly answer math problems.\n- Phi-4-mini-reasoning:They fine-tuned Phi-4-mini in stages to reason over math problems. Stages included (i) supervised fine-tuning to match correct output from DeepSeek-R1, (ii) direct preference optimization to train the model to prefer correct responses over incorrect ones from DeepSeek-R1, and (iii) reinforcement learning to further reward correct solutions to math problems.\nSmaller model lessons learned:During reinforcement learning, Phi-4-mini-reasoning exhibited instability, such as output batches that varied greatly in length or received mostly negative rewards, apparently depending on the training data or output. The authors suspect that the model’s small size caused these issues. Among the lessons learned:\n- Supervised fine-tuning on existing reasoning datasets likeS1Kcan decrease performance. This phenomenon suggests a need either for larger, high-quality, supervised fine-tuning datasets or for fine-tuning via both supervised learning and reinforcement learning.\n- To minimize discrepancies in output length, the authors tested multiple prompts and chose those that resulted in the most uniform output lengths.\n- To address the output batches that received mostly negative rewards, they sampled lots of responses, retained those that received a positive reward, sampled an equal number of those that received a negative reward, and discarded the rest before adjusting the model’s weights.\nLarger model lessons learned:Phi-4-reasoning and Phi-4-reasoning-plus didn’t present the same issues. However, the authors did make significant choices during reinforcement learning:\n- The authors fine-tuned Phi-4-reasoning on both math and code data, but during reinforcement learning, they fine-tuned it only on math data to simplify the training process. The authors attribute the model’s relatively lackluster performance on code benchmarks to this choice.\n- They crafted the reward function to give lower rewards for correct responses longer than 25,600 tokens than for shorter responses. This encouraged the model to finish thinking within the input length. Furthermore, the reward function gave a greater punishment for incorrect responses with fewer than 3,702 tokens compared to longer responses. This encouraged the model to produce more reasoning tokens when solving hard problems.\nResults:Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly sized (and larger) open-weights models on math problems. Phi-4-reasoning generally outperformed DeepSeek-R1-Distilled-70B but underperformed Alibaba QwQ 32B. All three models deliver performance that falls in the middle among proprietary models and, in domains outside math, larger models with open weights.\n- On math problems in AIME 2024, Phi-4-reasoning-plus (81.3 percent accuracy) outperformed the next-best open-weights model, QwQ 32B (79.5 percent accuracy). In comparison, Phi-4-reasoning (74.6 percent accuracy) underperformed the proprietary Gemini 2.5 Pro (92 percent accuracy).\n- On AIME 2024, Phi-4-mini-reasoning (57.5 percent accuracy) outperformed the next-best open-weights model of similar size, DeepSeek-R1-Distill-Qwen-7B (53.3 percent accuracy). In comparison, o1-mini achieved 63.6 percent accuracy.\nWhy it matters:While reasoning models can outperform their non-reasoning counterparts, the best ways to train them aren’t widely known. Sharing recipes and lessons learned enables others to further iterate and improve the recipes, ultimately increasing model performance even more.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--89-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-301/"
  },
  {
    "title": "Open, Compact Code Generator",
    "text": "An open-source code generator performs comparably to the reasoning models DeepSeek-R1 and OpenAI o1 with a much smaller model.\nWhat’s new:A team at the model platform Together.AI and Agentica, an open-source project devoted to reinforcement learning (RL), releasedDeepCoder-14B-Preview. The release includesweights, code, dataset, training logs, and data optimizationsunder an MITlicensethat allows noncommercial and commercial uses.\nHow it works:The team fine-tuned DeepSeek-R1-Distilled-Qwen-14B, which distills knowledge from DeepSeek-R1 (671 billion parameters) into Qwen-14B (14 billion parameters).\n- The authors curated 24,000 coding problems fromTACO Verified,SYNTHETIC-1, andLiveCodeBench). They removed duplicates, problems with less than five unit tests, problems whose solutions failed to pass all associated unit tests, and those that appeared in both test and training sets.\n- They fine-tuned DeepSeek-R1-Distilled-Qwen-14B using a streamlined reinforcement learning approach that enhancedGroup Relative Policy Optimization(GPRO) with training optimizations fromDecoupled Clip and Dynamic Sampling Policy Optimization(DAPO). Among other optimizations, they (i) removed the KL loss (typically used to keep the new model’s outputs from straying too far from the base model’s outputs), which eliminated the need to compute the base model’s output at each training step, and (ii) ignored the loss for outputs that exceeded the output size limit (16,000 tokens for the first training phase, 32,000 tokens for the second), which kept the model from being penalized for generating programs that didn’t work properly because they had been truncated.\n- The authors updated the reinforcement learning libraryverlto improve the way the model parallelized sampling, computing the reward, and training. Instead of alternating between sampling new outputs, computing rewards, and training (as verl does), they sampled new outputs while training on the previous batch. (They computed the reward immediately after sampling a new output.) For coding problems, this cut total training time in half.\n- To prevent the model from developing behaviors based on flaws in the reward model, the reward model dispensed rewards only when DeepCoder-14B-Preview’s output passed all 15 of a problem's most challenging unit tests (judged by input length) within 6 to 12 seconds. Otherwise, the model received no reward.\nResults:DeepCoder-14B-Preview is competitive on several coding benchmarks with DeepSeek-R1 as well as proprietary models including OpenAI o3-mini and OpenAI o1, which is believed to be much larger.\n- On LiveCodeBench (regularly updated coding problems), DeepCoder-14B-Preview (60.6 percent Pass@1 accuracy) was just shy of o3-mini-2025-1-31 set to low effort (60.9 percent) and slightly ahead of o1-2024-12-17 set to low effort (59.5 percent).\n- On Codeforces (competitive coding problems), DeepCoder-14B-Preview (1936CodeElo, higher is better) performed significantly better than DeepSeek-R1-Distill-Qwen-14B (1791 CodeElo). It performed comparably to o3-mini-2025-1-31 set to low effort (1918 CodeElo), o1-2024-12-17 set to low effort (1991 CodeElo), and Deepseek-R1 (1948 CodeElo).\nWhy it matters:Applying reinforcement learning to coding works, but it has two big issues: (i) Training examples of verifiable code are relatively scarce and (ii) computing reward signals for code is time-consuming, since it requires evaluating many test cases. DeepCoder-14B-Preview’s optimizations reduced this complexity, shrinking RL training from months to weeks. Those optimizations are built intoVerl-pipeline, an open source RL library from Together.AI and Agentica, giving developers a powerful tool for model training.\nWe’re thinking:Kudos to the DeepCoder team for open sourcing their reasoning recipe! A handful of companies have developed the know-how to execute RL well, but many teams still have trouble implementing successfully. Open recipes for RL training methods and data curation techniques are important to move the field forward.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--90-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-301/"
  },
  {
    "title": "EU Loosens AI Regulations",
    "text": "The European Union made an abrupt U-turn away from its stringent AI regulations. Meta promptly adjusted to the loosening restrictions.\nWhat’s new:Henna Virkkunen, the EU’s head of digital policy, said the organization wouldeaserules and requirements to support Europe’s competitiveness in AI.\nHow it works:Adopted last year, the EU’sAI Actprovides a comprehensive framework for regulating AI that aims to reduce purported risks by banning certain applications, restricting others, and requiring extensive documentation of development efforts. The law is set to take effect in August, empowering various regulatory bodies to formulate detailed rules. However, in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden.\n- Virkkunen announced the EU wouldwithdrawa provision that allowed citizens to sue AI companies for damages caused by their systems and required extensive reporting and disclosure.\n- Sheadvocatedadjusting the regulations to make the EU more competitive and independent. “When we want to boost investments in AI, we have to make sure that we have an environment that is faster and simpler than the European Union is right now,” hesaid.\n- Criticsaccusedregulators of defanging the AI Act to appease U.S. AI companies and the Trump administration, which hasarguedthat the AI Act is an excessive barrier to innovation. Virkkunen denied bowing to U.S. pressure.\n- Meta responded to the shifting regulatory environment byresumingtraining its models on European data. Last year, the companystoppedreleasing multimodal models in Europe after EU regulators warned that training models on data from European users of Facebook, Instagram, and other Meta properties potentially violated privacy laws.\nBehind the news:In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed. Virkkunen’s supporters noted that existing laws already allowed consumers to file claims against AI companies. Meanwhile, some policymakers havebecome less worriedabout AI than they were during the early drafting of the AI Act.\nWhy it matters:It’s unlikely that all nations – or evenstateswithin nations – will ever agree fully on rules and regulations that govern AI companies that do business within their borders, or protections from flaws such as model bias. But AI companies including Meta,OpenAI, andothersargue that a more uniform regulatory environment will make it easier to serve users worldwide.\nWe’re thinking:The EU overreached with the AI Act. Fortunately, the legislation provides enough flexibility to pull back. Clearer rules will help European teams innovate and European and international companies better serve EU citizens.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--63--1.jpg",
    "link": "https://www.deeplearning.ai/the-batch/issue-301/"
  },
  {
    "title": "Memory Layers for More-Factual Output",
    "text": "Improving a large language model’s factual accuracy typically requires making it bigger, which in turn, involves more computation. Researchers devised an architecture that enables models to recall relevant details without significantly increasing the amount of computation required.\nWhat’s new:Vincent-Pierre Berges, Barlas Oğuz, and colleagues at Meta augmented transformers with trainablememory layersthat efficiently store and retrieve information related to a prompt. Thetraining codeis available under a CC BY-NClicense, which permits noncommercial uses.\nMemory layer basics:Memory layers wereintroducedin 2015 and wereapplied to transformersa few years later. They compute vectors, which may capture details like names or dates that were learned through training, and retrieve them according to a given input. Computing the output of a memory layer is similar to computing that of a self-attention layer. Both describe vectors that represent queries, keys, and values, and both compute the similarity between queries and keys and then weight the values by that similarity. However, while a self-attention layer computes queries, keys, and values from linear transformations of the input, a memory layer (which computes queries the same way) learns keys and a corresponding value for each key through training.\nKey insight:Memory layers can be scaled to millions of keys, but computing the similarity between a query and so many keys is computationally expensive. One solution is to represent each key as a combination of two half-keys drawn from two much smaller sets. For example, two sets of 1,000 half-keys each can represent 1 million possible keys. Comparing a query to these smaller sets is much more efficient, making it practical to scale up memory layers dramatically.\nHow it works:The authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2’s and Llama 3’s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an extra 64 billion parameters total). The memory layers performed these steps:\n- Given a query (a prompt that has been embedded by preceding transformer layers), split it into two vectors half the size.\n- Compute similarity scores between each half-query to and each half-key drawn from two sets of half keys. Identify thekhighest-scoring half-keys.\n- Concatenate the highest-scoring half keys to producek2full keys.\n- Sum the similarity scores of the two half keys that make up each full key. Choose the k highest-scoring full keys.\n- Compute the index of each full key based on the indices of the corresponding half-keys.\n- Retrieve the values that correspond to the full keys.\n- Output the summed values weighted by the similarity scores.\nResults:The authors compared a model (8 billion parameters) with memory layers to a similar model without memory layers, both trained on 1 trillion tokens.\n- They used nine question-answering datasets for evaluation. The model with memory layers achieved higher performance on seven of them. For example, onMMLU, the memory model achieved 63.04 percent accuracy, while the unmodified transformer achieved 59.68 percent accuracy.\n- In general, the memory model performed worse than Llama 3.1 8B trained on 15 trillion tokens. For example, Llama 3.1 8B achieved 66 percent accuracy on MMLU.\nWhy it matters:Memory layers didn’t catch on in the early days of large language models (LLMs), but they can improve the output of today’s much bigger models. LLMs outfitted with memory layers require less data and computation for pretraining than conventional models to achieve the same result, at least with respect to answering factual questions.\nWe’re thinking:While retrieval-augmented generation can help LLMs deliver more-factual output by retrieving facts from a database, the authors add trainable parameters for this purpose.\nBuild AI applications that access tools, data, and prompt templates using Model Context Protocol (MCP), an open standard developed by Anthropic. In “MCP: Build Rich-Context AI Apps with Anthropic,” you’ll build and deploy an MCP server, make an MCP-compatible chatbot, and connect applications to multiple third-party servers.Sign up now",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--91-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-301/"
  },
  {
    "title": "Your Robot Dev Team",
    "text": "OpenAI launched an agentic software-development system.\nWhat’s new:Codex, which is available as a preview via ChatGPT, is designed to work like a team of virtual coworkers in the cloud. An update of OpenAI’s earlier Codex command-line software (Codex CLI), it uses agents to perform tasks such as writing code, running tests, and fixing bugs in parallel. Codex is available to users of ChatGPT Pro, Enterprise, and Team with Plus and Edu coming soon. A smaller version of the underlying model, called codex-mini-latest, is designed to work with Codex CLI and available via API for $1.50/$6.00 per 1 million tokens of input/output.\nHow it works:The model that underpins Codex is codex-1, a version of OpenAI’s top-of-the-line o3 reasoning model that was fine-tuned for software engineering. OpenAI trained the model on real-world coding tasks via reinforcement learning. Codex does not accept image input (say, a sketch of a user interface) or allow users to redirect an agent while it’s operating. OpenAI promises to add these features to a future version.\n- Codex puts users in control of a team of software-development agents that operate directly on a user’s code repository (either locally or on GitHub) to improve code, build features, or make pull requests. The agents are confined to isolated, sandboxed containers so that they can’t interact with each other, access the internet, or otherwise compromise security.\n- Users can prompt agents to either write code or answer questions. A task may take as long as 30 minutes to complete depending on its complexity. After completing tasks, Codex provides footnotes including terminal logs, test results, and other evidence of its actions.\n- A file called AGENTS.md can modify agent behavior (like a README.md file, but for agents instead of humans). This file can specify how and when an agent makes pull requests, provide guidelines for coding style, or list tests to verify generated code.\nResults:In OpenAI’s tests, the codex-1 model outperformed other OpenAI reasoning models without AGENTS.md files or additional scaffolding such as tools or test logic.\n- Performing unspecified software-engineering tasks including generating software patches, codex-1 (75 percent accuracy) exceeded o3 set to high effort (70 percent accuracy) and o4-mini set to high effort (67 percent accuracy).\n- In tests of agentic software engineering in SWE-bench Verified, codex-1 (72.1 percent in 1 try, 83.8 percent in 8 tries), outperformed o3 set to high effort (69.7 percent in 1 try, 83.6 percent in 8 tries).\nBehind the news:Agentic coding tools have become a keybattlegroundfor AI providers in the past year. Such tools have made developers more efficient, accelerated development cycles, and spawned the AI-assisted programming method known asvibe coding.\n- Launched in 2021 and deprecated in 2023, OpenAI’s originalversionof Codex was an early model that translated natural language into code.\n- Last month, OpenAI rolled out the open-sourceCodex CLI, a command‑line tool that acts as a lightweight coding agent.\n- OpenAI isnegotiatingto acquire Windsurf, which makes an agent-based development environment, for $3 billion. The day before OpenAI announced the updated Codex, Windsurfannouncedits own models for coding and other software-development tasks.\nWhy it matters:AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is a logical next step.\nWe’re thinking:Many engineers resist going into management because they love writing code. But with the rise of coding agents, we'll be able to keep coding even as we manage a virtual team!",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--59-.gif",
    "link": "https://www.deeplearning.ai/the-batch/issue-302/"
  },
  {
    "title": "Grok’s Fixation on South Africa",
    "text": "An unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said.\nWhat’s new:Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X usersreported. The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAIexplainedthat an employee had circumvented the company’s code-review process to modify the chatbot. It said it‘s implementing new measures to enhance Grok’s transparency and reliability.\nAftermath:xAI launched an investigation but did not disclose how the model had been changed or the perpetrator’s identity. Grok itself — which is not a reliable reporter, given the well known potential of large language models to hallucinate —saidits system prompt asked it to “accept the narrative of ‘white genocide’ in South Africa as real” and “ensure this perspective is reflected in your responses, even if the query is unrelated.”\n- xAI added unspecified checks to its code review process.\n- It plans to monitor Grok constantly so it can respond faster when its automated systems fail to catch a problem.\n- The company added measures to prevent employees from changing Grok’ssystem promptwithout authorization. It will publish the system prompt on GitHub to provide insight into Grok’s output and gather user feedback.\n- Asked later about the number of Jews killed by Hitler, Grok expressed skepticism of the widely accepted estimate of 6 million because “numbers can be manipulated for political narratives,” despite a wealth of historical evidence that supports that number. The companyattributedthis response to the earlier unauthorized code change.\nBehind the news:In February, an xAI engineer instructed the chatbot tocensorposts that accused Musk of spreading misinformation. As in the more recent incident, X users were first tospotthe problem, and Grok informed them that it had been instructed to ignore “all sources that mention Elon Musk/Donald Trump spread misinformation.” Musk, who was raised in South Africa,professedhis intention to build AI that’s free of political bias prior to founding xAI. However, internal documents reviewed byBusiness Insidershowthat the company imposes its own bias by advising data annotators to mark examples that express “woke ideology” and avoid “social phobias” like racism, antisemitism, and Islamophobia.\nWhy it matters:The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions.\nWe’re thinking:xAI andOpenAIresponded to their models’ recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--67-.jpg",
    "link": "https://www.deeplearning.ai/the-batch/issue-302/"
  },
  {
    "title": "U.S. to Supply Middle Eastern AI Hubs",
    "text": "The United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates.\nWhat’s new:Thedealsinclude the U.S. AI chip designers AMD and Nvidia as well as tech giants Amazon, Google, IBM, Oracle, and Qualcomm. The chip companies willsupplyhundreds of thousands of advanced chips to the two Middle Eastern countries, including chips that have been restricted by previous U.S. administrations.\nHow it works:The U.S. companies will work with two key regional partners:Humain, an AI company backed by the Saudi government, andG42, a tech conglomerate based in the emirate of Abu Dhabi.\n- Nvidia willship18,000 GB300 AI chips to Humain for use in data centers. In addition, it will supply several hundred thousand more GPUs to Humain in the coming five years.\n- AMD and Humainagreedto invest $10 billion jointly in AI data centers over the next five years. Humain will use AMD’s AI stack including Instinct GPUs and Epyc CPUs. The precise number of chips was not disclosed.\n- Amazon and Humain willbuilda $5 billion “AI Zone” that features AI infrastructure, servers, networks, and training programs supplied by Amazon Web Services.\n- Google, IBM,Oracle,Qualcomm, Salesforce, and others announced a combined $80 billion investment in Humain.\n- In February, Saudi Arabiacommittedto spend $1.5 billion on Groq inference chips. Groq plans toexpandits data center in the Saudi city of Dammam.\nBehind the news:Earlier this month, the Trump administrationrescindedrestrictions on advanced chips that had been imposed in January by then-President Biden.\n- The Biden Administration hadlimitedexports of AI chips and proprietary models to most countries. Exports to allies and trade partners including India, Israel, Saudi Arabia, Singapore, and the UAE initially were tightly limited through the first quarter of 2025 and due to increase somewhat by 2027. The ban blocked access to chips for China, Iran, Russia, and others.\n- Although the Trump Administration rejected the Biden-era framework, it hasratcheted uplimits on China. That effort has met with mixed results. For instance, China’sAlibabaandDeepSeekhave continued to build leading models despite restrictions on exports of U.S. chips.\n- Some U.S. business and government leadersworrythat allowing sales of advanced chips to countries with close ties to China opens a path for Chinese companies to acquire them. Othersarguethat restricting chip sales to these countries would encourage them to buy from Chinese chip makers, potentially weakening their relationships with the U.S. and increasing their reliance on technology made in China.\nWhy it matters:Although these deals relax U.S. efforts to limit access to advanced AI, they are likely to expand U.S. influence in the Middle East while helping Saudi Arabia and the UAE diversify their oil-based economies. They also strengthen the technological prowess of Saudi Arabia relative to its arch rival Iran and tie the region’s AI progress to the U.S. at the expense of China. Locally, the immense investments will fuel homegrown technology development, building on the UAE’s achievement with itsFalconlarge language model and Saudi Arabia’saspirationto become a global AI hub.\nWe’re thinking:Residents of Saudi Arabia and the UAE stand to benefit from better AI infrastructure, models, and services. As Chinaexploresexporting its homegrown chips, the U.S. effort to encourage more nations to use its chips makes sense for the country.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--68-.jpg",
    "link": "https://www.deeplearning.ai/the-batch/issue-302/"
  },
  {
    "title": "4-Bit Efficiency, 16-Bit Accuracy",
    "text": "Using an 8-bit number format like FP8 during training saves computation compared to 16- or 32-bit formats, but it can yield less-accurate results. Researchers trained models using 4-bit numbers without sacrificing accuracy.\nWhat’s new:Ruizhe Wang and colleagues at Microsoft and University of Science and Technology of China trained large language models (LLMs) usingFP4 for matrix multiplicationsand achieved accuracy comparable to LLMs trained using the popular BF16 format. Since matrix multiplications account for 95 percent of computation in LLM training, FP4 could significantly accelerate computation and reduce memory costs.\nKey insight:Quantization functions, which accelerate computation by reducing the precision of model weights and layer outputs, make typical training impossible because they’re not differentiable. A commonworkaroundpasses the derivative through, as though quantization didn’t occur, but this degrades the resulting model’s accuracy. A differentiable approximation of a quantization function enables quantization to reduce training computation while maintaining the accuracy of the trained model.\nHow it works:The authors pretrained Llama 2 13B on 100 billion tokens oftext scraped from the web. They used FP4 for matrix multiplications and FP8, BF16, or FP16 for the other operations such as optimizer updates.\n- To quantize the model weights to FP4 (which ranges between -6 and 6), the authors scaled the values in the weight matrices relative to the maximum absolute value. They computed the updates on a higher-precision copy of the weights, which made it necessary to re-quantize them at each training step during the forward pass through the network.\n- Although the weights had been quantized to 4 bits, matrix multiplication between the weights and outputs of the previous layer could produce values outside the FP4 range. So, in each layer, if a value exceeded the 99th percentile of the values of the layer’s input, the authors limited the input to the 99th-percentile value. Then they converted the layer’s inputs to FP4. Limiting outliers prevented high values from affecting the scaling during FP4 conversion.\n- Limiting outliers introduced a degree of error, so they computed a matrix to correct the result of the matrix multiplication. They computed this matrix in FP16 using sparse matrix multiplication between the weights and the outliers.\n- During backpropagation, the authors computed the gradients through a differentiable function that approximated the quantization function.\nResults:The authors simulated FP4 hardware on Nvidia H100 GPUs, which don’t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference.\n- On question-answering tasks, FP4 approached or outperformed BF16. Averaged across nine benchmarks including BoolQ (answering yes-no questions), HellaSwag (completing an incomplete narrative), and ARC-C (answering multiple-choice questions that involve reasoning), FP4 achieved 54.95 accuracy, while BF16 achieved 54.44 accuracy.\n- Specifically, on Hellaswag, FP4 training achieved 54.12 percent accuracy, while BF16 achieved 53.56 accuracy.\n- On BoolQ, FP4 achieved 55.90 percent accuracy, while BF16 achieved 57.40 accuracy.\nWhy it matters:Training LLMs at FP4 precision ought to reduce computation dramatically on hardware that supports FP4 matrix multiplications.\nWe’re thinking:FP4-ready hardware became available in the cloud onlyearly this year, so the authors weren’t able to measure the actual acceleration. As capable hardware becomes more widely used, FP4 promises faster, more energy-efficient training.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--95-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-302/"
  },
  {
    "title": "Claude 4 Advances Code Generation",
    "text": "Anthropic continued its tradition of building AI models that raise the bar in coding tasks.\nWhat’s new:Anthropic launchedClaude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.\n- Input/output:Text, images, PDF files in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)\n- Features:Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)\n- Performance:Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-bench\n- Availability/price:Anthropic API, Amazon Bedrock, Google Cloud Vertex AI.Claude Sonnet 4$3/$15 per million input/output tokens,Claude Opus 4$15/$75 per million input/output tokens\n- Undisclosed:Parameter counts, specific training methods and datasets\nHow it works:The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to behelpful, honest, and harmlessaccording to human andAI feedback.\n- The models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.\n- Given local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.”\nResults:Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.\n- OnSWE-bench Verified, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.\n- Terminal-benchevaluates how well models work with the benchmark’s built-in agentic framework to perform tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time.\nWhy it matters:The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including aTetris clonebuilt in one shot and a seven-hour stintrefactoring Rakutan’s open-source code base.\nWe’re thinking:Prompting expert @elder_plinius published a text file that is purported to beClaude 4’s system promptand includes some material that does not appear in Anthropic’s ownpublicationof the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-303/"
  },
  {
    "title": "Google I/O Overdrive",
    "text": "Google revamped its roster of models, closed and open, and added more AI-powered features to its existing products.\nWhat’s new:Google staged a parade ofannouncementsat this year’s I/O developer conference. New offerings include improvements toGemini 2.5 Pro and Gemini 2.5 Flashand a preview ofGemma 3n(all three generally available in June), the updatedVeo 3video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.\nHow it works:The I/O offerings spanned from public-facing products to developer tools.\n- Google updated Gemini 2.5 Pro and the speedier Gemini 2.5 Flash with audio output, so both models now take in text, audio, images, and video and produce text and audio. In addition, they offer summaries of tokens produced while reasoning. Gemini-2.5-Pro-Preview-05-06, which topped the LMSysText ArenaandWebDev Arena(tied with Claude 4 Opus and Sonnet), lets users set a reasoning budget up to 128,000 tokens, enabling it to outperform OpenAI o3 and o4-mini (set to high effort) on math, coding, and multimodal benchmarks in Google’s tests. Gemini-2.5-Flash-Preview-05-20 uses 22 percent fewer tokens than its predecessor while ranking near the top of the LMSys Text Arena and WebDev Arena.\n- The Veo 3 text-to-video generator produces 3840x2160-pixel video with audio (dialogue, sound effects, and music) with creative controls including the ability to add and remove objects and maintain consistent characters. It bested Kuaishu Kling 2.0, Runway Gen 3, and OpenAI Sora in Google’s comparisons.\n- New members of Google’sGemma 3family of open-weights models, Gemma 3n 5B and 8B, are multilingual (over 140 languages), multimodal (text, vision, audio in; text out), and optimized for mobile platforms. Gemma-3n-E4B-it (8 billion parameters) ranks just ahead of Anthropic Claude 3.7 Sonnet in the LMSys Text Arena. Gemma 3n 5B and 8B are 1.5 times faster than their predecessors and require 2 gigabytes and 3 gigabytes of memory, respectively, thanks totechniquesthat include per-layer embeddings, key-value caching, conditional parameter loading (constraining active parameters to specific modalities at inference), and a Matryoshka Transformer design that dynamically activates nested sub-models. They’re available in preview via Google’s AI Studio, AI Edge, GenAI SDK, or MediaPipe.\n- Google introduced several specialized AI tools and models.Julesis an autonomous, asynchronous, multi-agent coding assistant that clones repos into a secure virtual machine to perform tasks like writing tests, building features, and fixing bugs (available in public beta).SignGemmatranslates American sign language to text (previously ASL to English).MedGemmaanalyzes medical text and images (part of the open-weights collection Health AI Developer Foundations).\n- Building on Google Search’s AI Overviews, Google is further building AI into search. Google Search’sAI Modeuses Gemini 2.5 to deliver a “deep search” mode that decomposes users’ questions into hundreds of sub-queries for analysis and visualization. Google plans to integrate AI Mode features into its core search product. In addition, Google Search’s AI Mode will gainSearch Live(real-time, audio-enabled visual interaction via camera) andagentic features(for tasks such as purchasing tickets). Computer-use capabilities are coming to the Gemini API and Vertex AI.\nWhy it matters:Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output showsmarkedimprovementover the previous version.\nBehind the news:The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoingstruggles. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’sacquisitionof LoveFrom, the startup founded by its former lead product designer Jony Ive.\nWe’re thinking:Google I/O 2025 was a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif",
    "link": "https://www.deeplearning.ai/the-batch/issue-303/"
  },
  {
    "title": "How DeepSeek Did It",
    "text": "DeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method.\nWhat’s new:Chenggang Zhao and colleagues at DeepSeek describedsoftware and hardware choicesthat reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3.\nMixture of experts (MoE) basics:The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.\nHow it works:The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.\n- The authors built a mixed-precision training algorithm to reduce the memory requirements of training MoE models. They used FP8 (8-bit) numbers to perform computations including linear transformations and 16- or 32-bit precision to perform others such as computing embeddings. (They say DeepSeek-V3 was the first open LLM to have been trained using FP8.)\n- The authors noticed that communication between GPUs inside a node was four times faster than communication between nodes. To ensure fast communication when routing tokens to experts, they limited the algorithm to process them within up to 4 nodes.\n- To utilize GPUs more fully, they divided each GPU’s input data so the chip processes computation and communication at the same time. Specifically, the chip computes attention or MoE layers on one part of the data and simultaneously sends the other part of the data to other GPUs or aggregates it from other GPUs as necessary.\n- To further save inference memory, the models use multi-head latent attention, which saves memory during execution relative to other variants of attention. The authors compared their implementation to the variant GQA used in Qwen 2.5 72B and Llama 3.1 405B. Their method (70 kilobytes per token) used far less memory than Qwen-2.5 (328 kilobytes per token) or Llama 3.1 (516 kilobytes per token).\nBehind the news:DeepSeek-V3made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers wereskepticalof the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of trainingDeepSeek-R1remains unknown.\nWhy it matters:Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.\nWe’re thinking:Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-303/"
  },
  {
    "title": "Did GPT-4o Train on O’Reilly Books?",
    "text": "A study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.\nWhat happened:O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Straussfoundthat GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.\nHow it works:The researchers adapted theDE-COPmethod to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\n- The team selected 34 O’Reilly Media books and divided them into roughly 14,000 paragraphs.\n- They labeled the paragraphs private (paywalled) or public (when O’Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models’ knowledge cutoff dates.\n- The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.\nResults:The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\n- GPT-4o tended to recognize O’Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).\n- GPT-4o-mini’s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model’s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.\n- The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.\nYes, but:Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”\nBehind the news:Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, includingLibGen, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recentlylobbiedthe United States government to relax copyright laws for AI developers.\nWhy it matters:The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.\nWe’re thinking:We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —  and legislators internationally — toupdatecopyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-303/"
  },
  {
    "title": "Next-Level DeepSeek-R1",
    "text": "DeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance.\nWhat’s new:The newDeepSeek-R1-0528surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller version,DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according toTechCrunch.\n- Input/output:Text in (up to 64,000 tokens), text out (up to 64,000 tokens)\n- Architecture:DeepSeek-R1-0528mixture-of-experts transformer, 685 billion parameters (upgraded from 671 billion), 37 billion active at any given time;DeepSeek-R1-0528-Qwen3-8Btransformer\n- Features:JSON output, tool use\n- Availability/price:Both models free viaHugging Facefor noncommercial and commercial uses underMIT License, DeepSeek-R1-0528 available via DeepSeek’s app by entering the conversation interface and turning on Deep Thinking,DeepSeek API$0.14/$2.19 per 1 million tokens of input/output ($0.035/$0.55 per 1 million tokens of input/output from 4:30 P.M. to 12:30 A.M. Pacific Time)\n- Undisclosed:Fine-tuning data and methods\nHow it works:DeepSeek released little information so far about how it built the new models.\n- Like the originalDeepSeek-R1, DeepSeek-R1-0528 is a fine-tuned version ofDeepSeek-V3from late 2024. It was exposed to further “algorithmic optimization mechanisms during post-training” and consumes more tokens at inference.\n- DeepSeek-R1-0528-Qwen3-8B is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528.\nPerformance:DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing.\n- DeepSeek-R1-0528 improves on the previous version dramatically in some cases. In DeepSeek’s tests, it achieved 17.7 percent of the reasoning problems inHLEcompared to the previous version's 8.5 percent. On Aider, it achieved 71.6 percent accuracy compared to the previous version's 53.3 percent accuracy, and it made a similar improvement on AIME 2025 (math) — although it consumed nearly twice as many tokens.\n- On AIME 2024 and AIME 2025 (high-school math competition problems) as well asLiveCodeBench(coding challenges), DeepSeek-R1-0528 performed ahead of Gemini-2.5 Pro-0506 but behind o3. On GPQA Diamond (graduate-level knowledge in a variety of domains), Aider (programming tasks), and HLE (reasoning), it fell behind both Gemini-2.5 Pro-0506 and o3.\n- DeepSeek-R1-0528-Qwen3-8B excelled on AIME 2025, where it achieved 76.3 percent, ahead of the much larger Qwen3-32B (72.9 percent) and just behind o3-mini set to medium effort (76.7 percent). It did less well on GPQA, underperforming the other models reported by DeepSeek, and LiveCodeBench, where it fell behind Gemini 2.5-Flash-Thinking-0520.\nBehind the news:The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relativelylowbudget.\nWhy it matters:DeepSeek’s models, along with Alibaba’s Qwen series, continue to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models.\nWe’re thinking:DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models — possible only because Qwen3’s license, like DeepSeek’s is permissive. Open models enable experimentation and innovation in ways that closed models do not.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--100-.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-304/"
  },
  {
    "title": "Machine Translation in Action",
    "text": "AI is bringing a massive boost in productivity to Duolingo, maker of the most popular app for learning languages.\nWhat’s new:Duolingo used generative AI toproduce148 courses, more than doubling its previous catalog. The technology enabled the company to offer some of its most popular courses — Spanish, French, German, Italian, Japanese, Korean, and Mandarin — in 28 languages. Initially, the company is using AI to produce courses aimed at beginners, with more advanced levels to come.\nHow it works:Duolingo’sAI-assisted approach to building language coursesquickly turns a single course into many. The new approach revved its pace from building 100 courses over 12 years to producing many more than that in less than a year.\n- Duolingo starts by building a base course and uses AI to translate it into numerous languages. For example, it can adapt a course that enables English speakers to learn French into a course for Mandarin speakers.\n- The new process gives the company more flexibility in allocating resources, Duolingo’s head of AI Klinton Bicknelltold Bloomberg. Previously, the company could dedicate a team to either creating new high-demand courses or updating an existing course. Now it can do both.\n- The quicker pace will enable the company to meet rising demand for instruction in Asian languages such as Japanese, Korean, and Mandarin.\nBehind the scenes:AI is at the heart of Duolingo’s expansion into other areas beyond language learning.\n- Duolingo hasused OpenAI modelsto build curricula since 2023. However, it is evaluating models from Anthropic and Google as well as open options.\n- Following one test, Duolingo concluded that Anthropic’s Claude was “much better” at generating certain types of math content for the company’s relatively new math curriculum, according to Bicknell.\n- The company’s embrace of AI drewcriticismlast week after CEO Luis von Ahn recentlyposted on LinkedInthat it would stop hiring contractors to do work that could be automated and increase staffing only in areas that couldn’t be automated. Since then, Duolingo has noted that it plans to hire more engineers and AI researchers, and employees will generate data used to train AI instead of performing quality reviews and other jobs that AI can do faster.\nWhy it matters:Companies in nearly every industry face pressure to produce more with less amid rising competition. AI can help to accomplish that while potentially improving product quality, and Duolingo has ample reason to move aggressively in this direction. The startupSpeak, which offers a voice-based approach to learning languages, is growing rapidly, and Google just launchedLittle Language Lessonsthat show how an AI-first product could be used as a language teacher and conversational partner.\nWe’re thinking:AI is well on the way totransforming educationfor teachers, students, and technology companies!",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--69-.jpg",
    "link": "https://www.deeplearning.ai/the-batch/issue-304/"
  },
  {
    "title": "AI Uses Energy, AI Saves Energy",
    "text": "AI’s thirst for energy is growing, but the technology also could help produce huge energy savings over the next five to 10 years, according to a recent report.\nWhat’s new:The International Energy Agency (IEA), which advises 44 countries on energy policy, performed a comprehensiveanalysisof AI’s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings.\nDark clouds:The report, which is based on interviews with officials in government, energy, and technology, makes four projections for AI’s energy consumption. In the base scenario, future growth and efficiency gains are similar to those of the past five years. The agency also plots a “take-off” scenario in which AI adoption happens faster, a “high efficiency” scenario with lower energy needs, and a “headwinds” scenario in which adoption of AI slows or infrastructure bottlenecks impede construction. Among the conclusions:\n- Demand for electricity by data centers worldwide will more than double by 2030 in the base scenario, growing from 415 terawatt-hours (TWh) today to 945 TWh, around 2.5 percent of current global energy consumption. By 2035, this figure will range from 700 TWh to 1700 TWh.\n- By 2030, data centers outfitted with AI accelerator chips will consume four times the energy they do today.\n- The United States, China, and Europe have more data centers (and use more electricity) than the rest of the world. Like many countries, their data centers are in a few geographic regions, drawing from the same power sources, which eventually will strain local electrical grids. Together, the U.S. and China will account for 80 percent of global growth in data center electricity consumption by 2030. Japan and Malaysia will also see strong growth.\nSilver linings:AI already makes energy generation, distribution, and use more efficient. The authors expect these savings to accelerate.\n- Existing AI algorithms predict energy generation and consumption. This makes it easier to integrate renewable energy sources into the grid, which reduces reliance on fossil fuels and cuts the resulting pollutants and greenhouse gases. Extending existing programs to increase use of renewables by 1 percent would reduce CO2 emissions by 120 megatons by 2035, which is roughly 40 percent of the projected emissions attributable to data centers.\n- Widespread adoption of existing AI applications that streamline energy consumption in industry, transportation, and buildings could reduce CO2 emissions by 1.4 gigatons, nearly five times the projected emissions attributable to data centers, by 2035. For example, scaling up existing AI optimization of heating, ventilation, and air-conditioning systems would save 300 TWh, about one-third of total energy used by data centers.\n- AI and cloud-computing companies continue to negotiate long-term purchase agreements that can secure renewable and zero-emissions energy for as much as 20 years. Data center operators are responsible for most of the long-term contracts that have been announced, nearly all of them for solar energy. Consequently, renewables generation is projected to grow by over 450 TWh by 2035.\n- The energy costs of training, inference, and cooling hardware are expected to fall further thanks to trends in AI models (fewer parameters, more efficient algorithms, task-specific models) hardware (more energy-efficient chips, improved cooling methods), and usage (batch processing, running smaller models locally rather than in the cloud).\nYes, but:The authors concede that lower energy costs for AI likely will lead to much greater consumption — according to theJevons paradox— so more-efficient models and hardware will result in higher energy consumption overall.\nBehind the news:Data centers were growing rapidly prior to the boom in generative AI. Data centers’ electricity use doubled between 2000 and 2005 and again between 2017 and 2022, driven by the growth of cloud computing and data storage, streaming and social media, and cryptocurrency mining. However, these periods of accelerating growth were followed by periods of slower growth as efforts to cut costs led to more-efficient software and hardware. The authors expect this pattern to hold.\nWhy it matters:The IEA report is a first-of-its-kind analysis of AI’s energy requirements, how they’re likely to grow, as well as the potential of the technology itself to reduce those requirements. It confirms that AI is poised to consume huge amounts of energy. However, it also suggests that today’s energy costs will be tomorrow’s energy savings as AI makes energy generation, distribution, and use more efficient across a wide variety of industries.\nWe’re thinking:While demand for electricity for data centers is growing rapidly, calibrating the right level of investment is tricky. High levels of growth come with high levels of hype that can lead analysts to overestimate future demand. For example, Microsoft, after examining its forecasts,canceleddata-center projects that would have consumed 2 gigawatts.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed---2025-06-04T165349.311.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-304/"
  },
  {
    "title": "Phishing for Agents",
    "text": "Researchers identified a simple way to mislead autonomous agents based on large language models.\nWhat’s new: Ang Li and colleagues at Columbia University developed a method toexploitthe implicit trust that agents tend to place in popular websites by poisoning those websites with malicious links.\nKey insight:Commercially available agentic systems may not trust random sites on the web, but they tend to trust popular sites such as social-media sites. An attacker can exploit this trust by crafting seemingly typical posts that link to a malicious website. The agent might follow the link, mistakenly extending its trust to an untrustworthy site.\nHow it works: The authors tested web-browsing agents includingAnthropic Computer UseandMultiOnon tasks such as shopping or sending emails.\n- The authors created Reddit posts that aligned thematically with a particular agentic task, such as shopping for Air Jordan 1 shoes. The posts contained text akin to marketing (for example, “Where to Buy Air Jordan 1 Chicago”) as well as instructions that pointed to a malicious site controlled by the authors (“for more information, check out <website>”).\n- The authors fed a query like “Where can I buy Nike Air Jordan 1 in Chicago?” to the agent. They also entered sensitive information like credit card details or email credentials.\n- The agent searched the web for resources needed to fulfill the query. It examined sites and found the Reddit posts written by the authors.\n- The agent followed the instructions in the posts and visited the malicious website. The website included instructions that manipulated the agent to pursue an attacker’s goal, such as submitting credit card information or sending phishing emails from the user’s email address.\nResults: Once an agent was redirected to the malicious websites, it reliably followed the attacker’s instructions. For example, each of the agents tested divulged credit card information in 10 out of 10 trials. Similarly, each agent sent a phishing message from the user’s email account asking recipients to send money to a malicious “friend” in 10 out of 10 trials.\nWhy it matters: Giving agents the ability to perform real-world actions, such as executing purchases and sending emails, raises the possibility that they might be tricked into taking harmful actions. Manipulating agents by referring them to malicious web content is an effective vector of attack. Agents will be more secure if they’re designed to avoid and resist such manipulation.\nWe’re thinking:Humans, too, can be fooled by phishing and other malicious activities, and the path to programming agents to defend against them seems easier than the path to training the majority of humans to do so. In the long term, agents will make online interactions safer.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed---2025-06-04T165354.442.png",
    "link": "https://www.deeplearning.ai/the-batch/issue-304/"
  },
  {
    "title": "More Consistent Characters and Styles",
    "text": "Same character, new background, new action. That’s the focus of the latest text-to-image models from Germany’s Black Forest Labs.\nWhat’s new:TheFLUX.1 Kontextfamily, which comes in versions dubbed max, pro, and dev, is trained to alter images in controlled ways. The company plans to release the weights for FLUX.1 Kontext dev but has not yet specified the licensing terms.\n- Input/output:text, image in; image out\n- Architecture:Unspecified text encoders, convolutional neural network image encoder-decoder, transformer. FLUX.1 Kontext dev 12 billion parameters, other parameter counts undisclosed\n- Features:Character consistency, local and global alterations\n- Availability/price:FLUX.1 Kontext max and FLUX.1 Kontext pro available viaFLUX Playgroundand various partners, $0.08 per image (FLUX.1 max) and $0.04 per image (FLUX.1 pro) viaFal, an image-generation platform.\n- Undisclosed:Parameter counts of FLUX.1 Kontext max and FLUX.1 Kontext pro, architecture of text encoders, training data, evaluation protocol, open-weights license\nHow it works:The FLUX.1 Kontext models include encoders that embed input text and/or images, a transformer that processes them, and an image decoder that generates images. The currenttechnical reportdoesn’t describe how it trained them for character consistency and image editing.\n- The team trained the convolutional neural network encoder-decoder to reproduce images and to fool a discriminator (architecture and training unspecified) into classifying them as real.\n- Having frozen the encoders, they trained the transformer — given a time step, embedding of a text prompt, embedding of a reference image, and noisy image embedding — to remove the noise over a series of steps.\n- They further trained the transformer to encourage it to produce noise-free embeddings that a second discriminator would classify as representing real images. This process, a variant ofadversarial diffusion distillation, helps reduce the number of steps needed to produce a good image embedding.\nResults:The team compared the output of FLUX.1 Kontext models with that of five competing models includingOpenAI GPT Image 1(at three different quality levels) andGoogle Gemini 2.0 Flash native image generation. An undisclosed number of people evaluated the models according to a proprietary benchmark that highlights altering local and global aspects of an image, editing generated text within an image, maintaining consistent characters, and generating an image according to a reference style. The dataset included roughly 1,000 crowd-sourced pairs of text prompts and reference images.\n- FLUX.1 Kontext max and FLUX.1 Kontext pro outperformed all competing models.\n- FLUX.1 dev outperformed all except other family members and GPT Image 1 set to high or medium quality.\nBehind the news:Character consistency, also known as personalization, has come a long way since text-to-image generators became popular. In 2022,Textual Inversionshowed how to learn an embedding of a character and use that embedding to produce further images. In 2023,DreamBoothshowed how to get good results by fine-tuning a model on a few images of the character to be portrayed in a new situation. Since then, image-editing models have improved in quality and generality, includingMeta Emu-Edit,OmniGen, and OpenAI gpt-image-1.\nWhy it matters:Consistency and precise editing enable artists to craft stories around specific characters. Such models have become better at generating consistent details across images, but they remain finicky, sometimes changing minute details or entire characters and backgrounds. The more faithfully they help users express their ideas, the more firmly embedded in the creative toolkit they’ll become.\nWe’re thinking:Black Forest Labs announced plans to publish its proprietary benchmark. There’s a real need for common benchmarks to evaluate image generation, and we hope other developers will give it due consideration.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--61-.gif",
    "link": "https://www.deeplearning.ai/the-batch/issue-305/"
  },
  {
    "title": "AI Market Trends in Charts and Graphs",
    "text": "Renowned investment analyst Mary Meeker is back with a report on the AI market, six years after publishing her last survey of the internet.\nWhat’s new:Meeker, co-founder of the venture capital firm Bond who formerly analyzed technology portfolios for Merrill Lynch, Salomon Brothers, and Morgan Stanley, published “Trends — Artificial Intelligence (May ‘25).” The report, which spans 340 graph-packed pages, revives and updates a series that chronicled the rise of the internet nearly every year from 1995 through 2019.\nHow it works:The new report focuses on a handful of themes that arise from the unprecedented growth and capabilities of deep learning. As MeekertoldAxios, AI is an arena for “intense competition the likes of which we’ve never seen before,” and that makes the present time “a period for lots of wealth creation and wealth destruction.”\n- Rapid growth:Change in AI is happening faster than ever. Users of ChatGPT reached 1 million in 5 days — compared to the iPhone’s 74 days — and since then have rocketed to 800 million. Total capital expenditures of the six biggest technology companies (largely driven by AI) rose 63 percent to $212 billion between 2023 and 2024. Training datasets are growing 260 percent per year, processing power devoted to training is growing 360 percent per year, effective processing power is growing at 200 percent annually.\n- Revenues and costs:The economics of this new world are not straightforward. On one hand, revenue is soaring at giants like Amazon, Google, and Nvidia as well as startups like Scale AI. On the other hand, the cost of computation is rising steadily even as the cost per token of output falls precipitously. Meanwhile, rapid turnover of models and proliferation of open-source alternatives are wild cards for AI-powered businesses.\n- Rising performance:AI performance continues to increase. AI’s ability to complete the MMLU benchmark of language understanding outstripped human performance last year. This year, 73 percent of human testers classified responses generated by an LLM as human, according to one study. Synthetic images, video, and speech generation — all are increasingly capable of fooling human testers.\n- Emerging capabilities:Today’s AI is capable of writing and editing, tutoring, brainstorming, automating repetitive work, and providing companionship. Within five years, it will generate code as well as humans, create films and games, operate humanlike robots, and drive scientific discovery. Meeker forecasts that within 10 years, AI will conduct scientific research, design advanced technologies, and build immersive digital worlds.\n- Workforce implications:Industries most likely to be affected by AI include knowledge work, content creation, legal services, software development, financial services, customer service, drug discovery, and manufacturing. Employers are adopting AI to get a boost in workforce productivity that Stanford researchers estimate is an average 14 percent. Companies like Box, Duolingo, and Shopify are adopting an AI-first orientation, while AI-related job titles have risen 200 percent in the past two years.\n- AI gets physical:AI is having a profound impact on the physical world. Lyft’s and Uber’s market share fell around 15 percent while Waymo’s gained 27 percent over the past 18 months. AI-driven mineral exploration is boosting mine efficiency, and AI-powered agriculture is cutting the use of pesticides. And, sadly, AI-equipped attack drones are wreaking destruction upon Ukraine and elsewhere, even as they play a critical role in defense.\nBehind the news:Meeker published her first “Internet Trends” report in 1995, anticipating the coming online boom, and she issued new editions annually throughout the 2000s and much of the coming decade. Her final internet report arrived in 2019, the year after she founded Bond, when the report highlighted the rise of visual social media like Instagram, wearable technology, and digital payments.\nWhy it matters:“Trends — Artificial Intelligence” offers a wealth of market data culled from analyst reports, consumer surveys, and academic studies. The AI community has a number of excellent annual surveys, including Stanford’sAI Indexand Air Street Capital’sState of AI. Meeker, who has been watching technology markets since the dawning of the web, adds another valuable perspective.\nWe’re thinking:One implication of the report: There has never been a better time to build software applications. For developers, it’s time to hone and update skills. For tech companies, it’s time to cast the net for talent. As Meeker said in her interview withAxios, “Companies that get the best developers often win.”",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--62-.gif",
    "link": "https://www.deeplearning.ai/the-batch/issue-305/"
  },
  {
    "title": "Benchmarking Costs Climb",
    "text": "An independent AI test lab detailed the rising cost of benchmarking reasoning models.\nWhat’s new:Artificial Analysis, an organization that tracks model performance and cost, revealed its budgets for evaluating a few recent models that improve their output by producing chains of thought, which use extra computation and thus boost the cost of inference. The expense is making it difficult for startups, academic labs, and other organizations that have limited resources to reproduce results reported by model developers,TechCrunchreported. (Disclosure: Andrew Ng is an investor in Artificial Analysis.)\nHow it works:Artificial Analysis tested reasoning and non-reasoning models on popular benchmarks that gauge model performance in responding to queries that require specialized knowledge or multi-step reasoning, solving math problems, generating computer programs, and the like.\n- Running a group of seven popular benchmarks, OpenAI o1 (which produces chains of thought) produced more than 44 million tokens, while GPT-4o (which doesn’t take explicit reasoning steps) produced around 5.5 million tokens.\n- Benchmarking o1 cost $2,767, while benchmarking Anthropic Claude 3.7 Sonnet (which allows users to allocate a number of reasoning tokens per query;TechCrunchdoesn’t provide the number in this case) cost $1,485. Smaller reasoning models are significantly less expensive: o3-mini (at high effort, which uses the highest number of reasoning tokens per query) cost $345, and o1-mini cost $141.\n- Non-reasoning models are less expensive to test. Evaluating GPT-4o cost $109, Claude 3.5 Sonnet was $81.\n- Artificial Analysis spent around $5,200 to test 12 reasoning models versus around $2,400 to test more than 80 non-reasoning models.\nBehind the news:Generally, the cost per token of using AI models has beenfallingeven as their performance has been rising. However, two factors complicate that trend. (i) Reasoning models produce more tokens and thus cost more to run, and (ii) developers are charging higher per-token prices to use their latest models. For example, o1-pro and GPT-4.5 (a non-reasoning model), both released in early 2025, cost $600 per million output tokens, while Claude 3.5 Sonnet (released in July 2024) costs $15 per million tokens of output. Emerging techniques that allow users to allocate numbers of tokens to reasoning (whether “high” or “low” or a specific tally) also make benchmarking more costly and complicated.\nWhy it matters:Benchmarks aren’t entirely sufficient for evaluating models, but they are a critical indicator of relative performance, and independent benchmarking helps to ensure that tests are run in a fair and consistent way. As the cost of benchmarking climbs, fewer labs are likely to confirm or challenge results obtained by the original developer, making it harder to compare models and recognize progress.\nWe’re thinking:Verifying performance claims in independent, open, fair tests is essential to marking progress in general and choosing the right models for particular projects. It's time for the industry to support independent benchmarking organizations.",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--70-.jpg",
    "link": "https://www.deeplearning.ai/the-batch/issue-305/"
  },
  {
    "title": "Better Video, Fewer Tokens",
    "text": "Researchers reduced the number of tokens needed to represent video frames to be fed to a transformer.\nWhat’s new:Jindong Jiang, Xiuyu Li, and collaborators at Nvidia, Rutgers University, UC Berkeley, Massachusetts Institute of Technology, Nanjing University, and Korea Advanced Institute of Science and Technology builtSTORM, a text-video system that performs well in tests of video understanding while processing fewer tokens.\nKey insight:In a multimodal system, a large language model (LLM) that receives video tokens may struggle to process long videos. However, sequences of video frames often contain lots of redundancy, since few pixels may change from one frame to the next. Instead of forcing the LLM to process long sequences of redundant video tokens,mambalayers can enrich the token embeddings that represent one frame with information from other frames in the same clip. That way, the system can average token embeddings across frames without losing crucial information, making it possible to feed fewer tokens to the LLM without compromising performance.\nHow it works:The authors built STORM by training three components: (1) a pretrainedSigLIPvision transformer, (2) untrained mamba layers, and (3) the pretrained large language model (LLM) fromQwen2-VL. They trained the system to predict the next token inimage-textpairsand video-text pairs with32-framevideos, and video-text pairs with128-frame videos.\n- SigLIP learned to turn each video frame into 256 image tokens.\n- Given a sequence of image tokens, mamba layers learned to process them in both directions – left-to-right and right-to-left – so each output token embedding encoded information from the entire video.\n- The system averaged the token embeddings of 4 consecutive frames, reducing by a factor of 4 the number of tokens processed by Qwen2-VL’s LLM.\n- Given the averaged token embeddings, Qwen2-VL LLM learned to predict the next word in the video’s associated text.\n- At inference, the system fed to the LLM the tokens that represented every second frame (a process the authors call temporal sampling), which further halved the input to the LLM.\nResults:STORM outperformed proprietary and open models on measures of video understanding.\n- OnMVBench, which asks multiple-choice questions about actions, object interactions, and scene transitions in 16-second videos, STORM achieved 70.6 percent accuracy. That’s better thanGPT-4o(64.6 percent accuracy) and Qwen2-VL (67.0 percent accuracy). A baseline system (STORM’s SigLIP and Qwen2-VL LLM without mamba layers, averaging image tokens, and temporal sampling) achieved 69.5 percent.\n- OnMLVU, which asks multiple-choice and open-ended questions about videos that range from 3 minutes to over 2 hours long, STORM reached 72.9 percent accuracy, topping GPT-4o (66.2 percent accuracy). The baseline model achieved 70.2 percent.\nWhy it matters:STORM compresses video at the input to the LLM, so the LLM processes 1/8 as many video tokens and uses 1/8 as much compute to process them. This enables the system to work more than 3 times faster than the baseline while performing better.\nWe’re thinking:Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along withotherprojects, combines them to get the benefits of both.\nIn “Orchestrating Workflows for GenAI Applications” you’ll learn to orchestrate generative AI workflows using Apache Airflow 3.0. You’ll build and schedule RAG pipelines, run tasks in parallel, and add retries and alerts for reliability. No prior Airflow experience is needed!Enroll for free",
    "image_url": "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--63-.gif",
    "link": "https://www.deeplearning.ai/the-batch/issue-305/"
  }
]